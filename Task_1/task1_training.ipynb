{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task1: Training of the CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision as tv\n",
    "from torchvision import transforms, datasets\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch.optim as optim "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Settings\n",
    "path_train='./cats_and_dogs_filtered/train/'\n",
    "batch_size_train = 200 # how many samples per batch to load\n",
    "num_workers_train = 0 # subprocesses to use for data loading\n",
    "lr = 0.001 # TODO: learning rate\n",
    "size_compressed = [80,80] # size of each image after compression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'cats': 0, 'dogs': 1}\n"
     ]
    }
   ],
   "source": [
    "# Load training data\n",
    "\n",
    "train_transform = tv.transforms.Compose([\n",
    "    tv.transforms.Grayscale(num_output_channels=1), # reduce the channel Nr to one\n",
    "    tv.transforms.Resize(size_compressed),\n",
    "    tv.transforms.ToTensor()\n",
    "])\n",
    "train_data = tv.datasets.ImageFolder(root = path_train, transform = train_transform)\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size = batch_size_train,\n",
    "                                           num_workers = num_workers_train, shuffle = True)\n",
    "print(train_data.class_to_idx) # return: dict with items (class_name, class_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check some samples of our loaded training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of one training mini batch torch.Size([200, 1, 80, 80])\n",
      "Shape of one target mini batch torch.Size([200])\n",
      "Example training sample tensor([[[0.2941, 0.3059, 0.3176,  ..., 0.1255, 0.0471, 0.0196],\n",
      "         [0.3098, 0.3216, 0.3294,  ..., 0.1647, 0.1373, 0.0549],\n",
      "         [0.3176, 0.3294, 0.3373,  ..., 0.1216, 0.1569, 0.1490],\n",
      "         ...,\n",
      "         [0.3176, 0.3255, 0.3412,  ..., 0.3765, 0.3608, 0.3412],\n",
      "         [0.3098, 0.3216, 0.3333,  ..., 0.3647, 0.3490, 0.3294],\n",
      "         [0.3137, 0.3216, 0.3294,  ..., 0.3490, 0.3373, 0.3255]]])\n",
      "Target values tensor([0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1,\n",
      "        0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1,\n",
      "        0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1,\n",
      "        0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1,\n",
      "        1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0,\n",
      "        0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1,\n",
      "        1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1,\n",
      "        0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0,\n",
      "        1, 0, 1, 1, 1, 0, 0, 1])\n",
      "Dataset ImageFolder\n",
      "    Number of datapoints: 2000\n",
      "    Root location: ./cats_and_dogs_filtered/train/\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "               Grayscale(num_output_channels=1)\n",
      "               Resize(size=[80, 80], interpolation=PIL.Image.BILINEAR)\n",
      "               ToTensor()\n",
      "           )\n"
     ]
    }
   ],
   "source": [
    "examples = enumerate(train_loader)\n",
    "batch_idx, (example_data, example_targets) = next(examples)\n",
    "\n",
    "print('Shape of one training mini batch', example_data.shape)\n",
    "print('Shape of one target mini batch', example_targets.shape)\n",
    "print('Example training sample', example_data[0])\n",
    "print('Target values', example_targets[:])\n",
    "print(train_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ['cat', 'dog']\n",
    "mean, std = torch.tensor([0.5]), torch.tensor([0.5])\n",
    "\n",
    "def imshow_input(image):\n",
    "    image = image.permute(1, 2, 0)\n",
    "    image = torch.clamp(image,0, 1)\n",
    "    plt.imshow(image.squeeze(), cmap='gray') \t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current index of batch:0\n",
      "Shape of each image:torch.Size([1, 80, 80])\n"
     ]
    }
   ],
   "source": [
    "# get a batch of loaded training images\n",
    "image_batch = enumerate(train_loader)\n",
    "batch_idx, (images, labels) = next(image_batch)\n",
    "\n",
    "print('Current index of batch:{}'.format(batch_idx))\n",
    "print('Shape of each image:{}'.format(images[0].shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model of CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        # here we use a structure  contains 2 types of neural networks, a convolution network for the feature extraction\n",
    "        # and a fully connected layer to realize the classification \n",
    "        # ===========================================================================\n",
    "        # define the number of filters in the CNNs for features extraction\n",
    "        num_filters = [1, 64, 64, 128, 128]\n",
    "        # define the number of the nodes in the fully connected layers for classification\n",
    "        num_NN = [256, 64]\n",
    "        # compute a convolution layer ======== Some details https://pytorch.org/docs/stable/nn.html?highlight=conv2d#torch.nn.Conv2d\n",
    "        self.con1 = nn.Conv2d(in_channels = num_filters[0], \n",
    "                      out_channels = num_filters[1], \n",
    "                      kernel_size = 5, \n",
    "                      padding = 1,\n",
    "                      bias = False)\n",
    "        #compute amax pooling layer\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        #Batch nornamlization\n",
    "        self.bn1 = nn.BatchNorm2d(num_features=num_filters[1])\n",
    "        self.con2= nn.Conv2d(in_channels = num_filters[1],\n",
    "                      out_channels = num_filters[2], \n",
    "                      kernel_size = 3, \n",
    "                      padding = 1,\n",
    "                      bias = False)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.bn2 = nn.BatchNorm2d(num_features=num_filters[2])\n",
    "        self.con3 = nn.Conv2d(in_channels = num_filters[2],\n",
    "                      out_channels = num_filters[3], \n",
    "                      kernel_size = 3, \n",
    "                      padding = 1,\n",
    "                      bias = False)\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.bn3 = nn.BatchNorm2d(num_features=num_filters[3])\n",
    "        self.con4 = nn.Conv2d(in_channels = num_filters[3],\n",
    "                      out_channels = num_filters[4], \n",
    "                      kernel_size = 3, \n",
    "                      padding = 1,\n",
    "                      bias = False)\n",
    "        self.pool4 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.bn4 = nn.BatchNorm2d(num_features=num_filters[4])\n",
    "        self.fc1 = nn.Linear(num_filters[4]*4*4,num_NN[0])\n",
    "        self.fc2 = nn.Linear(num_NN[0],num_NN[1])\n",
    "        self.fc3 = nn.Linear(num_NN[1],2)\n",
    "    def forward(self, x):\n",
    "        x = self.pool1(F.relu(self.bn1(self.con1(x))))\n",
    "        x = self.pool2(F.relu(self.bn2(self.con2(x))))\n",
    "        x = self.pool3(F.relu(self.bn3(self.con3(x))))\n",
    "        x = self.pool4(F.relu(self.bn4(self.con4(x))))\n",
    "        x = x.view(-1, 128*4*4)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layout of the CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN(\n",
      "  (con1): Conv2d(1, 64, kernel_size=(5, 5), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (con2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (con3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  (pool3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (bn3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (con4): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  (pool4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (bn4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc1): Linear(in_features=2048, out_features=256, bias=True)\n",
      "  (fc2): Linear(in_features=256, out_features=64, bias=True)\n",
      "  (fc3): Linear(in_features=64, out_features=2, bias=True)\n",
      ")\n",
      "torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "kaggle_Net = CNN()\n",
    "print(kaggle_Net)\n",
    "params = list(kaggle_Net.parameters())\n",
    "print(params[2].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(NeuralNetwork,train_loader,loss_function,num_epochs, learning_rate=0.001, wd=0 ):\n",
    "    \"\"\"\n",
    "    Trains a neural network.\n",
    "    \n",
    "    NeuralNetwork = neural network to be trained\n",
    "    dataloader = DataLoader that deals batches for mini-batch learning\n",
    "    loss_function = cost function to be optimized\n",
    "    num_epochs = number of training epochs\n",
    "    l_rate = learning rate (default value 0.001)\n",
    "    wd = weight decay regularization (default value 0)\n",
    "    \"\"\"\n",
    "    optimizer = torch.optim.Adam(NeuralNetwork.parameters(), lr = learning_rate, weight_decay=wd) #use SGD as the optimizer \n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        errors = 0\n",
    "        for batch_idx , data in enumerate(train_loader,0):\n",
    "            inputs, labels = data\n",
    "            optimizer.zero_grad()\n",
    "            outputs = NeuralNetwork(inputs)\n",
    "            loss = loss_function(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            # error rate\n",
    "            predicted = torch.max(outputs,dim=1)\n",
    "            errors += sum(predicted[1] != labels)\n",
    "            if (batch_idx % 100) == 0:\n",
    "                print('Current loss ',running_loss/(batch_idx+1))\n",
    "        print('Epoch: ',epoch+1,'Error rate on training set:', round(100.0* errors.numpy() / len(train_loader.dataset),2), '%')\n",
    "    print('Finished Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current loss  4.262293815612793\n",
      "Epoch:  1 Error rate on training set: 53.7 %\n",
      "Current loss  0.6462950706481934\n",
      "Epoch:  2 Error rate on training set: 41.0 %\n",
      "Current loss  0.6566214561462402\n",
      "Epoch:  3 Error rate on training set: 33.7 %\n",
      "Current loss  0.6115033626556396\n",
      "Epoch:  4 Error rate on training set: 30.55 %\n",
      "Current loss  0.5776083469390869\n",
      "Epoch:  5 Error rate on training set: 25.7 %\n",
      "Current loss  0.4659102261066437\n",
      "Epoch:  6 Error rate on training set: 24.7 %\n",
      "Current loss  0.390491247177124\n",
      "Epoch:  7 Error rate on training set: 21.2 %\n",
      "Current loss  0.4465175271034241\n",
      "Epoch:  8 Error rate on training set: 16.95 %\n",
      "Current loss  0.36088693141937256\n",
      "Epoch:  9 Error rate on training set: 16.3 %\n",
      "Current loss  0.3342365026473999\n",
      "Epoch:  10 Error rate on training set: 13.2 %\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "train(kaggle_Net,train_loader,nn.CrossEntropyLoss(),10, lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(kaggle_Net,'kaggle_Net.pkl') #save the parameters of te net"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
