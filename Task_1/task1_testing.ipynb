{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1: Kaggle Cats and Dogs Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x11c255870>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision as tv\n",
    "from torchvision import transforms, datasets\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch.optim as optim \n",
    "\n",
    "# fix random seed for reproducability\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "path_test='./cats_and_dogs_filtered/validation/'\n",
    "batch_size_test = 100 \n",
    "num_workers_test = 0 \n",
    "size_compressed = [80,80]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Previous CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# before loading the model para, construct the CNN with the same structure at first\n",
    "class CNN(nn.Module):\n",
    "    \"\"\" \n",
    "    Create CNN model with convolutional net (for feature extraction) and \n",
    "    fully connected layers (for binary classification)\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        # define the number of neurons in convolutional layers\n",
    "        num_filters = [1, 64, 64, 128, 128]\n",
    "        # define the number of neurons in fully connected layers\n",
    "        num_neurons_fully = [256, 64]\n",
    "        \n",
    "        # compute the first convolution layer, some details see: [1]\n",
    "        # [1]: https://pytorch.org/docs/stable/nn.html?highlight=conv2d#torch.nn.Conv2d\n",
    "        # input.size([1,80,80])\n",
    "        self.con1 = nn.Conv2d(in_channels = num_filters[0], \n",
    "                      out_channels = num_filters[1], \n",
    "                      kernel_size = 5, \n",
    "                      padding = 1,\n",
    "                      bias = False)  # output.size([64, 77, 77])\n",
    "        # compute a max pooling layer\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2) # output.size([64, 39, 39])\n",
    "        # batch nornamlization\n",
    "        self.bn1 = nn.BatchNorm2d(num_features=num_filters[1])\n",
    "        \n",
    "        self.con2= nn.Conv2d(in_channels = num_filters[1],\n",
    "                      out_channels = num_filters[2], \n",
    "                      kernel_size = 3, \n",
    "                      padding = 1,\n",
    "                      bias = False)  # output.size([64, 38, 38])\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)  # output.size([64, 19, 19])\n",
    "        self.bn2 = nn.BatchNorm2d(num_features=num_filters[2])\n",
    "        \n",
    "        self.con3 = nn.Conv2d(in_channels = num_filters[2],\n",
    "                      out_channels = num_filters[3], \n",
    "                      kernel_size = 3, \n",
    "                      padding = 1,\n",
    "                      bias = False)  # output.size([128, 18, 18])\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)  # output.size([128, 9, 9])\n",
    "        self.bn3 = nn.BatchNorm2d(num_features=num_filters[3])\n",
    "        \n",
    "        self.con4 = nn.Conv2d(in_channels = num_filters[3],\n",
    "                      out_channels = num_filters[4], \n",
    "                      kernel_size = 3, \n",
    "                      padding = 1,\n",
    "                      bias = False)  # output.size([128, 8, 8])\n",
    "        self.pool4 = nn.MaxPool2d(kernel_size=2, stride=2)  # output.size([128, 4, 4])\n",
    "        self.bn4 = nn.BatchNorm2d(num_features=num_filters[4])\n",
    "        \n",
    "        self.fc1 = nn.Linear(num_filters[4]*4*4, num_neurons_fully[0], bias=True)\n",
    "        self.fc2 = nn.Linear(num_neurons_fully[0], num_neurons_fully[1], bias=True)\n",
    "        self.fc3 = nn.Linear(num_neurons_fully[1], 2, bias=True)\n",
    "    def forward(self, x):\n",
    "        x = self.pool1(F.relu(self.bn1(self.con1(x))))\n",
    "        x = self.pool2(F.relu(self.bn2(self.con2(x))))\n",
    "        x = self.pool3(F.relu(self.bn3(self.con3(x))))\n",
    "        x = self.pool4(F.relu(self.bn4(self.con4(x))))\n",
    "        x = x.view(-1, 128*4*4)  # flatten to 1D\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.sigmoid(self.fc3(x))\n",
    "        return x\n",
    "    \n",
    "kaggle_model = CNN()\n",
    "kaggle_model = torch.load('./kaggle_model_2.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\bullet$ Load test data and convert them to specific format (including changes of size, grayscale, number of channels and datatype) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'cats': 0, 'dogs': 1}\n"
     ]
    }
   ],
   "source": [
    "test_transform = tv.transforms.Compose([\n",
    "    tv.transforms.Grayscale(num_output_channels=1), # reduce the channel Nr. to one\n",
    "    tv.transforms.Resize(size_compressed),\n",
    "    tv.transforms.ToTensor()\n",
    "])\n",
    "test_data = tv.datasets.ImageFolder(root = path_test, transform = test_transform)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size = batch_size_test,\n",
    "                                           num_workers = num_workers_test, shuffle = True)\n",
    "print(test_data.class_to_idx) # return: dict with items (class_name, class_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error rate on test set: 22.6 %\n"
     ]
    }
   ],
   "source": [
    "errors_test = 0\n",
    "for batch_idx, (inputs,labels) in enumerate(test_loader):\n",
    "    outputs = kaggle_model(inputs)\n",
    "    # error rate\n",
    "    predicted = torch.max(outputs,dim=1)\n",
    "    errors_test += sum(predicted[1] != labels)\n",
    "print('Error rate on test set:', round(100.0* errors_test.numpy() / len(test_loader.dataset),2), '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
